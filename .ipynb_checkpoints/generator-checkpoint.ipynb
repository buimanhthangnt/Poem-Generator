{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import codecs\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "book_filenames = sorted(glob.glob(\"free_style.txt\"))\n",
    "corpus_raw = u\"\"\n",
    "for filename in book_filenames:\n",
    "    with codecs.open(filename, 'r', 'utf-8') as book_file:\n",
    "        corpus_raw += book_file.read()\n",
    "        \n",
    "corpus_raw = corpus_raw.replace('\\n\\n', '\\n')\n",
    "corpus_raw = corpus_raw.replace('\\n', ' ')\n",
    "\n",
    "corpus_raw = corpus_raw.lower()\n",
    "corpus_raw = corpus_raw.split()\n",
    "\n",
    "starting_words = []\n",
    "for idx, word in enumerate(corpus_raw):\n",
    "    if idx % 7 == 0:\n",
    "        starting_words.append(word)\n",
    "len(starting_words)\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    return np.random.choice(list(int_to_vocab.values()), 1, p=probabilities)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Graph and Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model.ckpt\n",
      "\n",
      "****************************\n",
      "tư duy lại lần hồi tuôn chảy\n",
      "những trang âm đêm hòa chìm sương\n",
      "\n",
      "yêu đương biết như em là yêu em chỉ là kẻ\n",
      "anh phải chỉ anh trái tim yêu một\n",
      "điều đã tim cô đứng hồi nắng\n",
      "em yêu anh câu ý cho bóng ấy\n",
      "em sẽ đừng anh sao em thương mãi bên bờ\n",
      "nên cuộc sống sao\n",
      "một ơi có thể san đi cuộc đời thể\n",
      "như mình anh bằng nhớ người xôi\n",
      "sao thương đã rầu mắt ngày được tình yêu\n"
     ]
    }
   ],
   "source": [
    "corpus_int, vocab_to_int, int_to_vocab, token_dict = pickle.load(open('preprocess.p', mode='rb'))\n",
    "seq_length, save_dir = pickle.load(open('params.p', mode='rb'))\n",
    "\n",
    "gen_length = 100\n",
    "prime_words = starting_words[np.random.randint(len(starting_words))]\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load the saved model\n",
    "    loader = tf.train.import_meta_graph(save_dir + '.meta')\n",
    "    loader.restore(sess, save_dir)\n",
    "    \n",
    "    # Get tensors from loaded graph\n",
    "    input_text = loaded_graph.get_tensor_by_name('input:0')\n",
    "    initial_state = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    sq_lengths = loaded_graph.get_tensor_by_name('sq_lengths:0')\n",
    "    final_state = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    probs = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    # Sentences generation setup\n",
    "    gen_sentences = prime_words.split()\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1 for word in gen_sentences]])})\n",
    "    \n",
    "    # Generate sentences\n",
    "    count = 0\n",
    "    sent = []\n",
    "    print(\"\\n****************************\")\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "        sql = [len(x) for x in dyn_input]\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state, sq_lengths: sql})\n",
    "        probabilities = probabilities[0]\n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "        gen_sentences.append(pred_word)\n",
    "        if pred_word != 'return':\n",
    "            sent.append(pred_word)\n",
    "        else:\n",
    "            print(' '.join(sent))\n",
    "            sent = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
